---
title: "IST707 Final Project"
author: "Zachary Chipman"
date: "6/21/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Rotten Tomatoes is a website where movies and TV shows are rated as either fresh
or rotten based on the percentage of positive reviews it receives. If more than 
60% of the critics wrote a positive review for a film, then it is rated fresh. 
Any less, and it's rated rotten. Films can also be rated as "Certified Fresh" if 
a certain number of what Rotten Tomatoes calls "Top Critics" give the film a 
positive review. 

Websites like Rotten Tomatoes give analysts the opportunity to see what factors 
determine the critical reception of the film. Machine learning can be used to 
not only see what attributes lead to a positive or negative review but can also
be used to predict critical reception. This project will explore a Kaggle data 
set containing the characteristics of a number of films as well as the critical
and audience reception from Rotten Tomatoes. Analysis will also be done for 
audience opinion so that it may compared to the critic analysis. 


## Analysis and Models

Note: Only important output/plots of the code will be displayed in the project 
proper. The full code with all outputs and plots will be included in the 
appendix of this paper.

### About the Data

The unaltered data set had 17,712 observations of 22 variables, some of which 
were removed during cleaning. Columns like the film's title were removed because 
they did not provide any relevant information for analysis while others like 
Director were removed because there were thousands of unique observations that 
would inhibit the generalization of the data.

There were also some columns that were removed because they would hurt the 
integrity of the analysis. These included the actual percentage of positive
reviews, known as CRating and ARating. A machine learning model would only have 
to look at these values to determine the status of the film. Since the goal of 
this experiment was to see what other factors might determine a film's 
reception, these columns were removed.  

Two text-based variables, Info and Consensus, were removed because they were out 
of this project's scope. These columns are a brief summary and the critical 
consensus of the film respectively and may be used in future research.

Below are descriptions of the variables that were used as well as the details of
how they were cleaned for analysis:

CStatus/AStatus (converted to factor): These are the variables that this paper 
will attempt to predict. CStatus can be either Certified Fresh, Fresh, or 
Rotten. AStatus can be either Upright or Spilled.

Content (converted to factor): The content rating that the Motion Picture 
Association gave the film due to its content. Every film in the data set had one 
of the following ratings:G, PG, PG-13, R, or NR (Not Rated). It should be noted 
that some of the films in the original data set were given the NC-17 rating but 
were removed due to a lack of observations.

Genre (converted to factor): Originally, the films could have multiple genres so 
the column was manipulated so only the first genre listed would be used. Genres 
that had less than 100 observations were removed to make running the models 
easier. The final 9 genres were: Action & Adventure, Animation, Art House & 
International, Classics, Comedy, Documentary, Drama, Mystery & Suspense,
and Horror. 

Release (Date): The release date to the film. From this variable, three other 
columns were created. The first was "Rdecade," which was the year of release 
binned by decade. The second was "Rmonth," which was the month the film was 
released. Finally, there was "Rmonth," which was the time of the month the film 
was released (either Early, Mid, or Late Month). All of these added columns were 
converted into factors.

Stream (Date): The date that the film was released on streaming services. This 
variable was also used to make three different columns that were similar to the 
ones made from the "Release" column. The one exception was that the stream year, 
"Syear," was not binned by the decade but rather by year (as the earliest year 
in the data was 1998).

Runtime (Date): The run time, in minutes, of the film. For the purpose of 
analysis, this column was binned by 1-hour bins and converted into an ordered 
factor.

After NAs were removed and all aforementioned data cleaning was also conducted,
the data set had 15,814 observations and 13 variables.

For the sake of brevity, only the graphs that show actionable insight into the 
data will be included into the paper. Charts for all the variables can be found
in the appendix. The color palate for these plots comes from a package called 
"wesanderson." Wes Anderson is a director whose films have a unique style all 
their own. This palette is inspired by his film "The Darjeeling Limited."

These graphs show that the majority of the films were rated positively (by both
critics and audience members). We also see that critics and audience members are
mostly in agreement on the overall status of the films. 

The majority of the films are either rated R or NR. 

As expected, the number of films in the data set increases by decade. The higher
proportion of Fresh to Rotten films in earlier decades makes sense since most of
those films are considered classics and are more likely to be rated favorably. 

One thing to note in these charts is that more films are released in January 
than any other month. This may be due because producers and directors want to
get theirs films noticed in time for award season. There is also surprisingly no
spike in the summer months when all of the major "blockbusters" usually come 
out. 

The vast majority of films are between 1-2 hours in length with a some falling 
between 2-3 hours. 

### Models

Two modeling methods will be used in this section, Association Rules and 
Classification. The former will be used to find out what characteristics make 
films more lauded by critics and/or audiences while the latter will be used to 
find a model that will most accurately predict how a future film will be 
received. Each of the models will be run twice, once for critical reception and 
again for audience reception. 

In both cases of the Association Rules mining, continuous variables are removed
to aid the analysis. The minimum confidence was set at 0.6 while the support was
tweaked until 20-30 strong rules were created. Below are is a summary of the 
rules mining. The code, as well as the all of the rules generated, can be found 
in the appendix.

#### Critic Association Rules

Critic Status = Fresh: 23 Rules Generated, Support ranges from 0.03 to 0.17, 
Confidence ranges from 0.6 to 0.75, Lift ranges from 1.64 to 2.02.

Critic Status = Rotten: 25 Rules Generated, Support ranges from 0.03 to 0.09, 
Confidence ranges from 0.6 to 0.71, Lift ranges from 1.38 to 1.62.

#### Audience Association Rules

Audience Status = Upright: 23 Rules Generated, Support ranges from 0.05 to 0.18,
Confidence ranges from 0.6 to 0.83, Lift ranges from 1.10 to 1.52.

Audience Status = Spilled: 26 Rules Generated, Support ranges from 0.03 to 0.08,
Confidence ranges from 0.6 to 0.83, Lift ranges from 1.33 to 1.84.

### Classification Models

The data was split using the two-thirds rule, with 2/3 of the data being part of
the training data while the remaining 1/3 was used as the testing data. Some of
of the columns were also removed to avoid redundancy and help the models run 
faster. The release and run time variables were removed in favor of their binned
counterparts. The variables related to streaming dates were also removed 
because, although they appeared in the association rules, there was no trend in 
the rules that signified a relation between them and a film's reception. 
Finally, it must be noted that the Critic/Audience Status column was removed 
from the test data and stored in a separate vectors called "ctestanswers" and 
"atestanswers" respectively. These vectors will be used in a confusion matrix to 
test the prediction models.

Caret's "trainControl" function will be used to train each of the models. The 
models will undergo 5-fold cross validation five times with a certain parameter
being tuned each time. The most accurate of these models will then be used for
analysis. As the function is a random function, the "set.seed" function was 
used to ensure reproducibility. Four training methods will be used: Decision 
Tree, Support Vector Machines (SVM), k-Nearest Neignbor (kNN), and Random 
Forest. The tuning parameters and the resulting accuracy are summarized in the 
following paragraphs. More detail can be found in the appendix. 

The decision tree model is tuned be the complexity parameter (cp), which 
controls the size of the tree. The final value used for the model was 
cp = 0.004389667, resulting in an accuracy of 0.58.

In SVM models, kernels are used to classify linearly inseparable data sets. This
model uses a radial kernel and is tuned by the cost parameter C, which is the 
penalty for an inaccurate prediction. The final value used for the model was 
C = 4, resulting in an accuracy of 0.58.

kNN is tuned by the the number of "nearest neighbors" that are considered during 
training (k). The final value used for the model was k = 13, resulting in an 
accuracy of 0.56.

Random Forest is tuned using "mtry." The Rdocumentation defines this parameter 
as the "number of variables randomly sampled as candidates at each split." This 
parameter has a more significant affect on the accuracy than the number of trees
does, which is set to 500 by default. The final value used for the model was 
mtry = 2, resulting in an accuracy of 0.57.

By plotting the accuracy of each model together, it can be seen that the SVM 
and decision tree models are the most accurate. 

When these models are applied to the test data, the accuracy is not much higher
(around 0.58-0.59). The results for all of the confusion matrices can be found 
in the appendix. 

#### Audience Classification Models

The same setup and parameters were used to predict the audience status. The 
decision tree had a cp = 0.005426357 and an accuracy of 0.67. The SVM had a 
C = 4 and an accuracy of 0.67. The kNN had a k = 13 and an accuracy of 0.65. 
Finally, the random forest had a mtry = 2 and an accuracy of 0.66. Similar to 
the CStatus models, the SVM and decision tree were slightly more accurate and 
none of the models saw a significant increase in accuracy when applied to the 
test data (0.65-0.67). However, the AStatus accuracy was higher for both the 
train and test data sets. The possible implications of this will be discussed 
in the following sections.

## Results

### Association Rules Mining

One trend that is hard to deny is that both critics and audiences favored films
that were not rated. Critics and audiences rated these films fresh over 60% of 
the time, with 17/23 and 16/23 rules respectively having Content = NR of the 
left-hand side. Both groups loved documentaries, with audiences rating them 
upright a whopping 83% of the time. Classics were, unsurprisingly, also highly 
favored, with a 71% confidence from critics and a 79% confidence from audiences.

This chart gives indication that there may be some overlap in these 
observations as the majority of classics and documentaries are unrated. 

The love of classics is self-explanatory, but the love of documentaries might be 
due to the fact that they are non-fiction, and therefore more real. It is not 
hard to understand or emphasize with the characters in a documentary because 
their experiences and struggles are genuine.

Comedy and Horror movies were the most unfavorably rated genres among both 
groups, with horror being especially panned by audiences (82% confidence). 
Horror films tend to have a reputation of being cheap and rely on gimmicks to
scare their audiences rather than tell a compelling story, which may explain 
this result.

Films rated PG-13 were not well received by critics, with 13/25 of the rules
having this characteristic. The rating of PG-13 was not introduced until the 
early 1980's, which may suggest that critics are less inclined to enjoy modern
films. However, other rules suggest that audiences may be even more critical of 
modern films as 16/26 of the rules include films that were released in the 
2010's. 

### Classification Models

The accuracy of the models was not extremely accurate. Both the train and test 
predictions were around 57-58% accurate regarding critic behavior while they
were slightly more accurate for audience behavior (65-67%). One theory is that 
while the status of critics could be one of three options the audience status 
could only one of two, therefore having less chance of an incorrect prediction.


## Conclusions

This project was a good start towards the goal of using machine learning to 
predict critic and audience behavior. However, time affected the scope and 
detail of the project so there are several steps that will be conducted in 
future research. 

Actions would be taken to improve the accuracy of the models. One of the issues 
may have been that there were too many genres in the data set for accurate 
classification to be conducted. If the focus was on what factors made a certain 
genre more favorable to critics and audiences, then a more accurate model may be 
obtained. 

As stated earlier, the text variables (Info and Consensus) were removed because
text mining was out of the scope of this project. But performing sentiment 
analysis (as well as other machine learning techniques) on these columns may 
provide insights that this project did not find. 

## Appendix

```{r}
# Libraries required for this project
library(plyr)
library(dplyr)
library(ggplot2)
library(readr)
library(caret) 
library(caretEnsemble) 
library(questionr)
library(klaR) 
library(e1071) 
library(rpart) 
library(rpart.plot) 
library(rattle) 
library(imager)
library(factoextra) 
library(randomForest)
library(arules)
library(arulesViz)
library(gridExtra)
library(wesanderson)

# Read in the data set (make sure that the cwd is correct)
movies <- read.csv("rotten_tomatoes_movies.csv", header = T, 
                   na.strings = c("", "NA"))
OGmovies <- movies

# Data Cleaning ############################################

# Rename Columns
movies <- plyr::rename(movies, c("rotten_tomatoes_link"="Link", 
                           "movie_title"="Title",
                           "movie_info"="Info",
                           "critics_consensus"="Consensus",
                           "content_rating"="Content",
                           "genres"="Genre",
                           "directors"="Director",
                           "authors"="Writer",
                           "actors"="Actor",
                           "original_release_date"="Release",
                           "streaming_release_date"="Stream",
                           "runtime"="Runtime",
                           "production_company"="Production",
                           "tomatometer_status"="CStatus",
                           "tomatometer_rating"="CRating",
                           "tomatometer_count"="CCount",
                           "audience_status"="AStatus",
                           "audience_rating"="ARating",
                           "audience_count"="ACount",
                           "tomatometer_top_critics_count"="Top",
                           "tomatometer_fresh_critics_count"="Fresh",
                           "tomatometer_rotten_critics_count"="Rotten"))
 
# Remove all columns that are not needed for analysis.
movies <- subset(movies, select = -c(Link, Title, Director, Writer, Actor, 
                 Production, CRating, CCount, ARating, ACount, Top, Fresh, 
                 Rotten))

# Modify the genre column so that it only shows the first genre listed.
movierows <- nrow(movies)
for (x in 1:movierows) {
  movies$Genre[x] <- gsub("(.*?),.*", "\\1", movies$Genre[x])
}

# Covert nominal variables into factors
movies$Content = factor(movies$Content)
movies$Genre = factor(movies$Genre)
movies$CStatus = ordered(movies$CStatus, levels = c("Rotten", "Fresh", 
                                                    "Certified-Fresh"))
movies$AStatus = ordered(movies$AStatus)


# Remove columns with lots of text (may use them for NLP if time).
movietext <- subset(movies, select = c(Info, Consensus))
movies <- subset(movies, select = -c(Info, Consensus))

# Check for and remove NAs
length(movies[movies=='NA'])
movies <- na.omit(movies)
length(movies[movies=='NA'])

# Removes Levels with less than 100 values
movies <- movies[movies$Content != "NC17",]
movies <- movies[movies$Genre != "Cult Movies",]
movies <- movies[movies$Genre != "Kids & Family",]
movies <- movies[movies$Genre != "Musical & Performing Arts",]
movies <- movies[movies$Genre != "Romance",]
movies <- movies[movies$Genre != "Science Fiction & Fantasy",]
movies <- movies[movies$Genre != "Special Interest",]
movies <- movies[movies$Genre != "Western",]

movies$Content=droplevels(movies$Content)
movies$Genre=droplevels(movies$Genre)

# Convert Release Date and Stream Date to Date Format
movies$Release <- as.Date(movies$Release , format = "%Y-%m-%d")
movies$Stream <- as.Date(movies$Stream , format = "%Y-%m-%d")

# Discretize Release and Stream dates for EDA and analysis.
Ryear <- as.numeric(format(movies$Release, "%Y"))
Rdecade <- c()
for (x in 1:length(Ryear)) {
  Rdecade[x] <- Ryear[x] - Ryear[x] %% 10
}
Syear <- as.numeric(format(movies$Stream, "%Y"))

abbmonth <- ordered(c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", 
                      "Sep","Oct", "Nov", "Dec"))

abbmonth <- ordered(abbmonth, levels = c("Jan", "Feb", "Mar", "Apr", "May", 
                                         "Jun", "Jul", "Aug", "Sep","Oct", 
                                         "Nov", "Dec") )
Rmonth <- as.numeric(format(movies$Release, "%m"))
Rmonth <- abbmonth[Rmonth]
Smonth <- as.numeric(format(movies$Stream, "%m"))
Smonth <- abbmonth[Smonth]

Rday <- as.numeric(format(movies$Release, "%d"))
Rdaybin <- ifelse(Rday <= 31 & Rday > 20, "Late Month", 
                  ifelse(Rday <= 20 & Rday > 10, "Mid-Month", "Early Month"))
Rdaybin <- ordered(Rdaybin, levels = c("Early Month", "Mid-Month", 
                                       "Late Month"))
Sday <- as.numeric(format(movies$Stream, "%d"))
Sdaybin <- ifelse(Sday <= 31 & Sday > 20, "Late Month", 
                  ifelse(Sday <= 20 & Sday > 10, "Mid-Month", "Early Month"))
Sdaybin <- ordered(Sdaybin, levels = c("Early Month", "Mid-Month", 
                                       "Late Month"))
dates <- data.frame(Rdecade, Syear, Rmonth, Smonth, Rdaybin, Sdaybin)

movies <- cbind(movies, dates)

# Discretize Runtime for EDA and analysis.
Runtimebin <- ifelse(movies$Runtime > 240, "240+", ifelse(movies$Runtime <= 240 
                                   & movies$Runtime > 180, "181-240", 
                                   ifelse(movies$Runtime <= 180 
                                          & movies$Runtime > 120, "121-180",
                                          ifelse(movies$Runtime <= 120
                                                 & movies$Runtime > 60, 
                                                 "61-120", "1-60"))))
Runtimebin <- ordered(Runtimebin, levels = c("1-60", "61-120", "121-180",
                                             "181-240", "240+"))
movies <- cbind(movies, Runtimebin)

# EDA ######################################################

# Status
gcstatus <- ggplot(movies, aes(x=CStatus)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Critic Status") 
gcstatus <- gcstatus + xlab("Status") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
gastatus <- ggplot(movies, aes(x=AStatus)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Audience Status") 
gastatus <- gastatus + xlab("Status") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
gstatus <- grid.arrange(gcstatus, gastatus, nrow=1)
gstatus

# Content Rating
ccontent <- ggplot(movies, aes(x=Content)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("MPA Rating (Crtitc)") 
ccontent <- ccontent + xlab("Rating") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
acontent <- ggplot(movies, aes(x=Content)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("MPA Rating (Audience)") 
acontent <- acontent + xlab("Rating") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
gcontent <- grid.arrange(ccontent, acontent, nrow=1)
gcontent

# Genre
cgenre <- ggplot(movies, aes(x=Genre)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Genre (Critic)") 
cgenre <- cgenre + xlab("Genre") + ylab("Frequency") + 
  labs(fill = "Critic Status") + theme(axis.text.x = element_text(angle = 90)) 
cgenre <- cgenre + scale_fill_manual(values = wes_palette(n=3, 
                                                         name="Darjeeling1"))
agenre <- ggplot(movies, aes(x=Genre)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Genre (Audience)") 
agenre <- agenre + xlab("Genre") + ylab("Frequency") + 
  labs(fill = "Audience Status") + theme(axis.text.x = element_text(angle = 90))
agenre <- agenre + scale_fill_manual(values = wes_palette(n=3,  
                                                          name="Darjeeling1"))
ggenre <- grid.arrange(cgenre, agenre, nrow=1)
ggenre

# Release Decade 
cdecade <- ggplot(movies, aes(x=Rdecade)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Decade Released") 
cdecade <- cdecade + xlab("Decade") + ylab("Frequency") + 
  labs(fill = "Critic Status")  + scale_fill_manual(values = wes_palette(n=3, 
                                                   name="Darjeeling1"))
adecade <- ggplot(movies, aes(x=Rdecade)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Decade Released") 
adecade <- adecade + xlab("Decade") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
gdecade <- grid.arrange(cdecade, adecade, nrow=1)
gdecade

# Stream Year
cyear <- ggplot(movies, aes(x=Syear)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Stream Year (Critic)") 
cyear <- cyear + xlab("Year") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
ayear <- ggplot(movies, aes(x=Syear)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Stream Year (Audience)") 
ayear <- ayear + xlab("Year") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
gyear <- grid.arrange(cyear, ayear, nrow=1)
gyear

# Release Month
crmonth <- ggplot(movies, aes(x=Rmonth)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Month Released (Critic)") 
crmonth <- crmonth + xlab("Month") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
armonth <- ggplot(movies, aes(x=Rmonth)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Month Released (Audience)") 
armonth <- armonth + xlab("Month") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
grmonth <- grid.arrange(crmonth, armonth, nrow=1)
grmonth

# Stream Month
csmonth <- ggplot(movies, aes(x=Smonth)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Month Streamed (Critic)") 
csmonth <- csmonth + xlab("Month") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
asmonth <- ggplot(movies, aes(x=Smonth)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Month Streamed (Audience)") 
asmonth <- asmonth + xlab("Month") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
gsmonth <- grid.arrange(csmonth, asmonth, nrow=1)
gsmonth

# Release Day
crday <- ggplot(movies, aes(x=Rdaybin)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Time of Month Released (Critic)") 
crday <- crday + xlab("Time of Month") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
arday <- ggplot(movies, aes(x=Rdaybin)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Time of Month Released (Audience)") 
arday <- arday + xlab("Time of Month") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
grday <- grid.arrange(crday, arday, nrow=1)
grday

# Stream Day
csday <- ggplot(movies, aes(x=Sdaybin)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Time of Month Streamed (Critic)") 
csday <- csday + xlab("Time of Month") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
asday <- ggplot(movies, aes(x=Sdaybin)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Time of Month Streamed (Audience)") 
asday <- asday + xlab("Time of Month") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
gsday <- grid.arrange(csday, asday, nrow=1)
gsday

# Runtime
crtime <- ggplot(movies, aes(x=Runtimebin)) + 
  geom_bar(aes(fill = CStatus)) + ggtitle("Runtime (Critic)") 
crtime <- crtime + xlab("Runtime (mins)") + ylab("Frequency") + 
  labs(fill = "Critic Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                  name="Darjeeling1"))
artime <- ggplot(movies, aes(x=Runtimebin)) + 
  geom_bar(aes(fill = AStatus)) + ggtitle("Runtime (Audience)") 
artime <- artime + xlab("Runtime (mins)") + ylab("Frequency") + 
  labs(fill = "Audience Status") + scale_fill_manual(values = wes_palette(n=3, 
                                                    name="Darjeeling1"))
grtime <- grid.arrange(crtime, artime, nrow=1)
grtime

# Association Rules ########################################

# Critics
crules <- subset(movies, select = -c(Release, Stream, AStatus, Release, Stream, 
                                     Runtime))

crules$Rdecade = as.factor(crules$Rdecade)
crules$Syear = as.factor(crules$Syear)

# I tweaked the numbers till I got between 20 and 30 rules 
Freshrules <- apriori(data = crules, parameter=list(supp=0.03,conf=0.6),
                  appearance = list(default="lhs", rhs="CStatus=Fresh"), 
                  control=list(verbose=F))
summary(Freshrules)
Freshrules <- sort(Freshrules, decreasing = TRUE, by="lift")
inspect(Freshrules[1:23])

Rottenrules <- apriori(data = crules, parameter=list(supp=0.03,conf=0.6),
                      appearance = list(default="lhs", rhs="CStatus=Rotten"), 
                      control=list(verbose=F))
summary(Rottenrules)
Rottenrules <- sort(Rottenrules, decreasing = TRUE, by="lift")
inspect(Rottenrules[1:25])

# Audience
arules <- subset(movies, select = -c(Release, Stream, CStatus, Release, Stream, 
                                                             Runtime))
arules$Rdecade = as.factor(arules$Rdecade)
arules$Syear = as.factor(arules$Syear)

Uprightrules <- apriori(data = arules, parameter=list(supp=0.045,conf=0.6),
                      appearance = list(default="lhs", rhs="AStatus=Upright"), 
                      control=list(verbose=F))
summary(Uprightrules)
Uprightrules <- sort(Uprightrules, decreasing = TRUE, by="lift")
inspect(Uprightrules[1:23])

Spilledrules <- apriori(data = arules, parameter=list(supp=0.025,conf=0.6),
                       appearance = list(default="lhs", rhs="AStatus=Spilled"), 
                       control=list(verbose=F))
summary(Spilledrules)
Spilledrules <- sort(Spilledrules, decreasing = TRUE, by="lift")
inspect(Spilledrules[1:26])


# Classification ###########################################

numTotalMovies = nrow(movies)
trainRatio <- .66
set.seed(11)
sample <- sample.int(n = numTotalMovies, 
                     size = floor(trainRatio*numTotalMovies), replace = FALSE)
train <- movies[sample, ]
test <- movies[-sample, ]
length(sample)/nrow(movies)

# Removing unnecessary columns for classification
ctrain <- subset(train, select = -c(Release, Stream, Runtime, AStatus, Syear,
                                     Smonth, Sdaybin))
ctest <- subset(test, select = -c(Release, Stream, Runtime, AStatus, Syear,
                                    Smonth, Sdaybin, CStatus))
ctestanswers <- test$CStatus

# Removing unnecessary columns for classification
atrain <- subset(train, select = -c(Release, Stream, Runtime, CStatus, Syear,
                                    Smonth, Sdaybin))
atest <- subset(test, select = -c(Release, Stream, Runtime, CStatus, Syear,
                                  Smonth, Sdaybin, AStatus))
atestanswers <- test$AStatus

# Creating a control with cross validation of 5
control <- trainControl(method ='cv',number = 5)

# Metric for comparison will be accuracy for this project
metric <-  "Accuracy"

set.seed(10)

# Decision Tree
tree.model <- train(CStatus ~ ., data = ctrain, method="rpart", metric=metric, 
                    trControl=control,
                    tuneLength = 5)

# Accuracy of Decision Tree Model
print(tree.model)

tree_acc = 58
Accuracy <- data.frame(tree_acc)

# Plotting decision tree model
plot(tree.model, main = "Decision Tree")

# Support Vector Machine (SVM)
svm.model <- train(CStatus ~ ., data = ctrain, method="svmRadial",metric=metric,
                   trControl=control, tuneLength = 5)

# Accuracy of Support Vector Machine (SVM)
print(svm.model)

# Plotting Support Vector Machine Model
plot(svm.model, main = "SVM")

svm_acc = 58
Accuracy <- data.frame(cbind(Accuracy,svm_acc))

# kNN
knn.model <- train(CStatus ~ ., data = ctrain, method="knn", metric=metric, 
                   trControl=control, tuneLength = 5)
# Accuracy of kNN model
print(knn.model)

kNN_acc = 57
Accuracy <- data.frame(cbind(Accuracy,kNN_acc))

# Plotting kNN model
plot(knn.model, main = "kNN")

# Random Forest
# Default number of trees is 500
set.seed(10)
rf.model <- train(CStatus ~ ., data = ctrain, method="rf", metric=metric,
                  trControl=control, tuneLength = 5)

# Accuracy of Random Forest Model
print(rf.model)

rf_acc = 57
Accuracy <- data.frame(cbind(Accuracy,rf_acc))

# Plotting random forest model
plot(rf.model, main = "Random Forest")

# summarize accuracy of models
results <- resamples(list(Decesion_Tree=tree.model, knn=knn.model,
                          SVM=svm.model,Random_Forest=rf.model))

dotplot(results)

# Prediction 

# Prediction on the test data using decision tree
dt <- predict(tree.model, ctest)

prediction <- data.frame(dt)

# Prediction on the test data using svm
svm <- predict(svm.model, ctest)

prediction <- data.frame(cbind(prediction, svm))

# Prediction on the test data using knn
knn <- predict(knn.model, ctest)

prediction <- data.frame(cbind(prediction, knn))

# Prediction on the test data using random forest
random_f <- predict(rf.model, ctest)

#prediction <- data.frame(random_f)
prediction <- data.frame(cbind(prediction, random_f))

Accuracy

# Critic Confusion Matricies
confusionMatrix(prediction$dt, ctestanswers)
confusionMatrix(prediction$svm, ctestanswers)
confusionMatrix(prediction$knn, ctestanswers)
confusionMatrix(prediction$random_f, ctestanswers)

# Audience 

# Decision Tree
tree.model <- train(AStatus ~ ., data = atrain, method="rpart", metric=metric, 
                    trControl=control,
                    tuneLength = 5)

# Accuracy of Decision Tree Model
print(tree.model)

tree_acc = 68
Accuracy <- data.frame(tree_acc)

# Plotting decision tree model
plot(tree.model, main = "Decision Tree")

# Support Vector Machine (SVM)
svm.model <- train(AStatus ~ ., data = atrain, method="svmRadial",metric=metric,
                   trControl=control, tuneLength = 5)

# Accuracy of Support Vector Machine (SVM)
print(svm.model)

# Plotting Support Vector Machine Model
plot(svm.model, main = "SVM")

svm_acc = 67
Accuracy <- data.frame(cbind(Accuracy,svm_acc))

# kNN
knn.model <- train(AStatus ~ ., data = atrain, method="knn", metric=metric, 
                   trControl=control, tuneLength = 5)

# Accuracy of kNN model
print(knn.model)

kNN_acc = 64
Accuracy <- data.frame(cbind(Accuracy,kNN_acc))

# Plotting kNN model
plot(knn.model, main = "kNN")

# Random Forest
# Default number of trees is 500
set.seed(10)
rf.model <- train(AStatus ~ ., data = atrain, method="rf", metric=metric,
                  trControl=control, tuneLength = 5)

# Accuracy of Random Forest Model
print(rf.model)

rf_acc = 66
Accuracy <- data.frame(cbind(Accuracy,rf_acc))

# Plotting random forest model
plot(rf.model, main = "Random Forest")

# summarize accuracy of models
results <- resamples(list(Decesion_Tree=tree.model, knn=knn.model,
                          SVM=svm.model,Random_Forest=rf.model))

dotplot(results)

# Prediction on the test data using decision tree
dt <- predict(tree.model, atest)

prediction <- data.frame(dt)

# Prediction on the test data using svm
svm <- predict(svm.model, atest)

prediction <- data.frame(cbind(prediction, svm))

# Prediction on the test data using knn
knn <- predict(knn.model, atest)

prediction <- data.frame(cbind(prediction, knn))

# Prediction on the test data using random forest
random_f <- predict(rf.model, atest)

# prediction <- data.frame(random_f)
prediction <- data.frame(cbind(prediction, random_f))

# Audience Confusion Matricies
confusionMatrix(prediction$dt, atestanswers)
confusionMatrix(prediction$svm, atestanswers)
confusionMatrix(prediction$knn, atestanswers)
confusionMatrix(prediction$random_f, atestanswers)

# Plot for results section
rulesplot <- ggplot(movies, aes(x=Genre)) + 
  geom_bar(aes(fill = Content)) + ggtitle("Content Rating by Genre") 
rulesplot <- rulesplot + xlab("Genre") + ylab("Frequency") + 
  labs(fill = "Content Rating") + theme(axis.text.x = element_text(angle = 90)) 
rulesplot <- rulesplot + scale_fill_manual(values = wes_palette(n=5, 
                                                            name="Darjeeling1"))
rulesplot
```

